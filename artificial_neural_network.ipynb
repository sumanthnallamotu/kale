{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg",
    "tags": []
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L",
    "tags": []
   },
   "source": [
    "### imports step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z",
    "tags": []
   },
   "source": [
    "### functions step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "# Defining the functions that will be used to create and train the model\n",
    "def load_data(url):\n",
    "    if url != None:\n",
    "        dataset = pd.read_csv(url, error_bad_lines=False)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def encode_gender(toChange):\n",
    "    le = LabelEncoder()\n",
    "    toChange[:,2] = le.fit_transform(toChange[:,2])\n",
    "    \n",
    "    return toChange\n",
    "\n",
    "\n",
    "def encode_geo(toChange):\n",
    "    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "    toChange = np.array(ct.fit_transform(toChange))\n",
    "    \n",
    "    return toChange\n",
    "\n",
    "\n",
    "def split_data(x, y, testSize, randomState):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=testSize, random_state=randomState)\n",
    "    return [x_train, x_test, y_train, y_test]\n",
    "\n",
    "\n",
    "def feature_scale(train, test):\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(train)\n",
    "    x_test = sc.fit_transform(test)\n",
    "    \n",
    "    return [x_train, x_test, sc]\n",
    "\n",
    "\n",
    "def build_ann(unit1, actFunc, unit2, actFunc2, opt, lossFunc, metric):\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(tf.keras.layers.Dense(units=unit1, activation=actFunc))\n",
    "    ann.add(tf.keras.layers.Dense(units=unit1, activation=actFunc))\n",
    "    ann.add(tf.keras.layers.Dense(units=unit2, activation=actFunc2))\n",
    "    ann.compile(optimizer = opt, loss = lossFunc, metrics = metric)\n",
    "    \n",
    "    return ann\n",
    "\n",
    "def confMat(x, y):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load_data step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "block:load_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n",
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset from my github\n",
    "dataset = load_data(\"https://raw.githubusercontent.com/sumanthnallamotu/kale/master/Churn_Modeling.csv\")\n",
    "x = dataset.iloc[:,3:-1].values\n",
    "y = dataset.iloc[:,-1].values\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6bQ0UgSU-NJ",
    "tags": []
   },
   "source": [
    "### encoding_data step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "block:encode_data",
     "prev:load_data"
    ]
   },
   "outputs": [],
   "source": [
    "# Encode categorical data\n",
    "x = encode_gender(x)\n",
    "x = encode_geo(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHol938cW8zd",
    "tags": []
   },
   "source": [
    "### split_data step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "block:split_data",
     "prev:encode_data"
    ]
   },
   "outputs": [],
   "source": [
    "# Split the dataset into the training and test sets and then feature scale them\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, 0.2, 0)\n",
    "x_train, x_test, sc = feature_scale(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zfEzkRVXIwF",
    "tags": []
   },
   "source": [
    "## build_ann step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "block:build_ann",
     "prev:split_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples\n",
      "Epoch 1/20\n",
      "8000/8000 [==============================] - 1s 68us/sample - loss: 0.6594 - accuracy: 0.6296\n",
      "Epoch 2/20\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 0.4746 - accuracy: 0.8018\n",
      "Epoch 3/20\n",
      "8000/8000 [==============================] - 0s 36us/sample - loss: 0.4458 - accuracy: 0.8077\n",
      "Epoch 4/20\n",
      "8000/8000 [==============================] - 0s 42us/sample - loss: 0.4286 - accuracy: 0.8111\n",
      "Epoch 5/20\n",
      "8000/8000 [==============================] - 0s 41us/sample - loss: 0.4149 - accuracy: 0.8183\n",
      "Epoch 6/20\n",
      "8000/8000 [==============================] - 0s 52us/sample - loss: 0.4017 - accuracy: 0.8296\n",
      "Epoch 7/20\n",
      "8000/8000 [==============================] - 0s 55us/sample - loss: 0.3897 - accuracy: 0.8382\n",
      "Epoch 8/20\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3806 - accuracy: 0.8441\n",
      "Epoch 9/20\n",
      "8000/8000 [==============================] - 0s 62us/sample - loss: 0.3731 - accuracy: 0.8472\n",
      "Epoch 10/20\n",
      "8000/8000 [==============================] - 0s 48us/sample - loss: 0.3672 - accuracy: 0.8503\n",
      "Epoch 11/20\n",
      "8000/8000 [==============================] - 0s 46us/sample - loss: 0.3630 - accuracy: 0.8505\n",
      "Epoch 12/20\n",
      "8000/8000 [==============================] - 0s 41us/sample - loss: 0.3593 - accuracy: 0.8516\n",
      "Epoch 13/20\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3567 - accuracy: 0.8525\n",
      "Epoch 14/20\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3551 - accuracy: 0.8525\n",
      "Epoch 15/20\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 0.3535 - accuracy: 0.8537\n",
      "Epoch 16/20\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3526 - accuracy: 0.8543\n",
      "Epoch 17/20\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3518 - accuracy: 0.8515\n",
      "Epoch 18/20\n",
      "8000/8000 [==============================] - 0s 36us/sample - loss: 0.3513 - accuracy: 0.8525\n",
      "Epoch 19/20\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3504 - accuracy: 0.8533\n",
      "Epoch 20/20\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3499 - accuracy: 0.8521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x118fc8828>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the model\n",
    "ann = build_ann(5, 'relu', 1, 'sigmoid', 'adam', 'binary_crossentropy', ['accuracy'])\n",
    "ann.fit(x_train, y_train, batch_size = 32, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJj5k2MxZga3",
    "tags": []
   },
   "source": [
    "## prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "block:prediction",
     "prev:build_ann"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Any input for a the prediction method should be a 2D array\n",
    "# You also have to standardize the input by calling sc.transform()\n",
    "# You could use something like > 0.5 to give a straight answer based on the prediction\n",
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)\n",
    "\n",
    "y_pred = ann.predict(x_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test), 1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0oyfLWoaEGw",
    "tags": []
   },
   "source": [
    "### confusion_matrix step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "block:confusion_matrix",
     "prev:prediction"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1506   89]\n",
      " [ 201  204]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix tells you how many predictions from the model were correct\n",
    "# The model correctly predicted that 1529 would stay in the bank and incorrectly predicted that 66 would stay\n",
    "# The model correctly predicted that 200 would leave the bank and incorrectly predicted that 205 would stay\n",
    "cm = confMat(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6ae2643a094a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(cm[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "gcr.io/arrikto/jupyter-kale:v0.5.0-47-g2427cc9",
   "experiment": {
    "id": "new",
    "name": "ownmodel1"
   },
   "experiment_name": "ownmodel1",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "ann1",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-pipeline-test-h5154ulq0",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
